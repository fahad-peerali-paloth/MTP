# -*- coding: utf-8 -*-
"""SampleCNNRNNtrain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MCWAFig8XjVGMXarJgSYKH-_e3nbbdcS
"""

import tensorflow as tf
tf.config.run_functions_eagerly(True)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.models import Model
from keras.layers import Input, Conv1D, LSTM, Dense, Flatten, GRU

csv_file_path = 'lstm_P_x.csv'
df = pd.read_csv(csv_file_path)

X1 = df.iloc[:, 2:6].values
Y = df.iloc[:, 6:19].values
X2 = df.iloc[:, 19:22].values
X = np.concatenate([X1, X2], axis=1)

num_samples = len(X)
num_samples -= num_samples % 8

X = X[:num_samples]
Y = Y[:num_samples]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)


X_train_scaled = X_train
X_test_scaled = X_test
Y_train_scaled = Y_train/(1025*9.81*0.3134) #change the normalisation factor wrt particulars
Y_test_scaled = Y_test/(1025*9.81*0.3134)


time_steps = 1
X_train_lstm = np.reshape(X_train_scaled, (X_train_scaled.shape[0], time_steps, X_train_scaled.shape[1]))
X_test_lstm = np.reshape(X_test_scaled, (X_test_scaled.shape[0], time_steps, X_test_scaled.shape[1]))

X_train.shape

input_layer = Input(shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))


conv_layer = Conv1D(filters=64, kernel_size=1, activation='relu')(input_layer)
gru_layer1 = GRU(units=128, activation='relu', return_sequences=True)(conv_layer)
gru_layer2 = GRU(units=128, activation='relu', return_sequences=True)(gru_layer1)
gru_layer3 = GRU(units=64, activation='relu')(gru_layer2)


output_layer = Dense(Y_train_scaled.shape[1])(gru_layer3)


model = Model(inputs=input_layer, outputs=output_layer)
model.summary()

X_train[1,4:7]

model.compile(optimizer='adam', loss=custom_loss)


import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
batch_size = 8
history = model.fit(X_train_lstm, Y_train_scaled, epochs=100, batch_size=batch_size, validation_data=(X_test_lstm, Y_test_scaled))

y_pred_scaled = model.predict(X_test_lstm)
y_pred = y_pred_scaled*(1025*9.81*0.3134)
Y_test = Y_test_scaled*(1025*9.81*0.3134)
rmse = sqrt(mean_squared_error(Y_test, y_pred))

print(f"RMSE: {rmse:.2f}")

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

for i in range(13):
    plt.figure(figsize=(10, 6))
    plt.plot(Y_test[120:250, i], label='Actual Data', color='blue')
    plt.plot(y_pred[120:250, i], label='Predicted Data', color='red')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.title(f'Actual vs. Predicted Data (Variable {i})')
    plt.legend()
    plt.show()

import joblib
modelrff = joblib.load('random_forest_model.joblib')

import pickle
with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(modelrff, file)

from google.colab import files
files.download('random_forest_model.pkl')

import tensorflow as tf
from tensorflow.keras.losses import Huber
from tensorflow.keras import backend as K
from tensorflow.keras.losses import MeanSquaredError


def custom_loss(y_true, y_pred):
    dfx = pd.DataFrame(y_true)
    Y_train_scaled1 = tf.cast(Y_train_scaled, dtype=tf.float32)
    y_true1 = tf.cast(y_true, dtype=tf.float32)
    all_indices = tf.TensorArray(tf.int64, size=0, dynamic_size=True)
    for i in range(len(y_true)):
      indices= tf.where(tf.reduce_all(tf.equal(Y_train_scaled1, y_true1[i,:]), axis=1))
      all_indices = all_indices.write(i, indices)
    result_indices = all_indices.concat()
    res=pd.DataFrame(result_indices)
    row_numbers = res.values.ravel()
    x_true_pre = np.zeros((len(res), 3))
    x_true_pre[:, 0:3] = X_train[row_numbers, 4:7]
    beta=[-0.070216,-0.01124,-0.010027,-0.00802,0,1.404,4.564,11.132,15.583,29.095,42.782,49.051,50.16]
    max_value = np.zeros((len(x_true_pre), len(beta)))
    for k in range(len(x_true_pre)):
      for i in range(len(beta)):
        pxtest = [x_true_pre[k, 0], x_true_pre[k, 1], x_true_pre[k, 2], beta[i]]
        pxtest = np.array(pxtest)
        pxtest = pxtest.reshape(1, -1)
        max_value[k,i] = modelrff.predict(pxtest)

    threshold = 0.7 * max_value

    loss_within_threshold = MeanSquaredError()(y_true, y_pred)
    penalty = tf.where(y_true > threshold, tf.square(y_true - y_pred), 0.0)
    loss_beyond_threshold = tf.reduce_sum(penalty)
    print("Penalty:",loss_beyond_threshold)
    total_loss = loss_within_threshold + loss_beyond_threshold

    return total_loss

from tensorflow.python.ops.array_ops import zeros
import tensorflow as tf
from keras import backend as K

def custom_loss_combined(y_true, y_pred):
    num_components = K.int_shape(y_true)[-1]
    P1_true=y_true[:,0]
    P2_true=y_true[:,1]
    P3_true=y_true[:,2]
    P4_true=y_true[:,3]
    P5_true=y_true[:,4]
    P6_true=y_true[:,5]
    P7_true=y_true[:,6]
    P8_true=y_true[:,7]
    P9_true=y_true[:,8]
    P10_true=y_true[:,9]
    P11_true=y_true[:,10]
    P12_true=y_true[:,11]


    P1_pred=y_pred[:,0]
    P2_pred=y_pred[:,1]
    P3_pred=y_pred[:,2]
    P4_pred=y_pred[:,3]
    P5_pred=y_pred[:,4]
    P6_pred=y_pred[:,5]
    P7_pred=y_pred[:,6]
    P8_pred=y_pred[:,7]
    P9_pred=y_pred[:,8]
    P10_pred=y_pred[:,9]
    P11_pred=y_pred[:,10]
    P12_pred=y_pred[:,11]

    L1_loss = K.mean(K.square(y_true - y_pred))

    L_combined = L1_loss

    return L_combined

from tensorflow import keras
model.save('BEM-GRUcnnf.h5')
from google.colab import files
files.download('BEM-GRUcnnf.h5')



import tensorflow as tf
from tensorflow.keras.losses import Huber
from tensorflow.keras.callbacks import Callback

class UpdateDeltaCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 5 == 0:
            new_delta = self.model.get_layer('delta_layer').get_weights()[0] * 1.2
            self.model.get_layer('delta_layer').set_weights([new_delta])


max_value = 100

initial_delta = 1.0

threshold = 0.7 * max_value

def custom_loss(y_true, y_pred):
    delta_layer = tf.keras.layers.Lambda(lambda x: initial_delta, name='delta_layer')(y_true)
    huber_loss = Huber(delta=delta_layer)
    max_value = 100
    threshold = 0.7 * max_value
    loss_within_threshold = huber_loss(y_true, y_pred)
    penalty = tf.where(y_true > threshold, tf.square(y_true - y_pred), 0.0)
    loss_beyond_threshold = tf.reduce_sum(penalty)
    total_loss = loss_within_threshold + loss_beyond_threshold
    return total_loss

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=1, input_shape=(4,)))
model.compile(optimizer='adam', loss=custom_loss)

update_delta_callback = UpdateDeltaCallback()
model.fit(X_train, Y_train, epochs=20, callbacks=[update_delta_callback])